{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import monai\n",
    "from monai.data import list_data_collate, decollate_batch, partition_dataset, DatasetSummary\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    AsChannelLastd,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    ConcatItemsd,\n",
    "    LoadImaged,\n",
    "    RandRotated,\n",
    "    EnsureTyped,\n",
    "    AddChanneld,\n",
    "    CropForegroundd,\n",
    "    EnsureType,\n",
    "    Spacingd,\n",
    "    CenterSpatialCropd,\n",
    "    SpatialPadd,\n",
    "    ScaleIntensityRanged,\n",
    "    SqueezeDimd,\n",
    "    ScaleIntensityd,\n",
    "    NormalizeIntensityd\n",
    ")\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from monai.metrics.utils import do_metric_reduction\n",
    "from monai.inferers import SimpleInferer\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from networks import Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data directory and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir ='/path/data'\n",
    "\n",
    "file_name_flair = 'FLAIR_1.nii.gz'\n",
    "file_name_t1 = 'T1_1.nii.gz'\n",
    "file_name_seg = 'seg_les_1.nii.gz'\n",
    "\n",
    "all_patients = os.listdir(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset for training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "partitions = partition_dataset(data=all_patients, ratios=[0.8,0.2],shuffle=True, seed=1234)\n",
    "train_folders = partitions[0]\n",
    "val_folders = partitions[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read image filenames for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_flair_images = [os.path.join(root_dir,i,file_name_flair) for i in train_folders]\n",
    "train_t1_images = [os.path.join(root_dir,i,file_name_t1) for i in train_folders]\n",
    "train_segs = [os.path.join(root_dir,i,file_name_seg) for i in train_folders]\n",
    "\n",
    "\n",
    "val_flair_images = [os.path.join(root_dir,i,file_name_flair) for i in val_folders]\n",
    "val_t1_images = [os.path.join(root_dir,i,file_name_t1) for i in val_folders]\n",
    "val_segs = [os.path.join(root_dir,i,file_name_seg) for i in val_folders]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of dictionaries containing the path to images and segmentation for each exam \n",
    "\n",
    "train_files = [{\"img\": gt, \"t1\": t1, \"seg\": seg} for gt, t1, seg in zip(train_flair_images, train_t1_images, train_segs)]\n",
    "val_files = [{\"img\": gt, \"t1\": t1, \"seg\": seg} for gt, t1, seg in zip(val_flair_images, val_t1_images, val_segs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the transformations to be applied on the image data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transforms = Compose(\n",
    "    [   LoadImaged(keys=['img','t1','seg'], reader='ITKReader'),\n",
    "         AddChanneld(keys=['img','t1','seg'],),\n",
    "         Spacingd(keys=['img','t1','seg'],pixdim=(0.8,0.8,1), mode=['bilinear', 'bilinear', 'nearest']),\n",
    "         CropForegroundd(keys=['img','t1','seg'],source_key='seg', margin=10),\n",
    "         CenterSpatialCropd(keys=['img','t1','seg'],roi_size =(192,240,160)),\n",
    "         SpatialPadd(keys=['img','t1','seg'],spatial_size=(192,240,160)),\n",
    "         RandRotated(keys=['img','t1','seg'],range_x=0.2, prob=0.1, mode=['bilinear', 'bilinear', 'nearest']),\n",
    "         NormalizeIntensityd(keys=['img','t1']),\n",
    "         ScaleIntensityRanged(keys='seg', a_min=0, a_max=5, b_min=0, b_max=1),\n",
    "         ConcatItemsd(keys=['t1','seg'],name='input'),\n",
    "         EnsureTyped(keys=['img','t1','seg']),\n",
    "    ]\n",
    ")\n",
    "    \n",
    "val_transforms = Compose(\n",
    "    [   LoadImaged(keys=['img','t1','seg'], reader='ITKReader'),\n",
    "         AddChanneld(keys=['img','t1','seg'],),\n",
    "         Spacingd(keys=['img','t1','seg'],pixdim=(0.8,0.8,1), mode=['bilinear', 'bilinear', 'nearest']),\n",
    "         CropForegroundd(keys=['img','t1','seg'],source_key='seg', margin=10),\n",
    "         CenterSpatialCropd(keys=['img','t1','seg'],roi_size =(192,240,160)),\n",
    "         SpatialPadd(keys=['img','t1','seg'],spatial_size=(192,240,160)),\n",
    "         NormalizeIntensityd(keys=['img','t1']),\n",
    "         ScaleIntensityRanged(keys='seg', a_min=0, a_max=5, b_min=0, b_max=1),\n",
    "         ConcatItemsd(keys=['t1','seg'],name='input'),\n",
    "         EnsureTyped(keys=['img','t1','seg']),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loaders for training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=list_data_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "train_loader_iter = iter(train_loader) # create iterable object for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=4, collate_fn=list_data_collate)\n",
    "post_trans = Compose([EnsureType()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize images and corresponding segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_loader_iter)\n",
    "img, seg = batch['img'], batch['input']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "for j in range(1):\n",
    "        plt.subplot(2, 3, j + 1)\n",
    "        plt.imshow(img[j,0,:, :, 90], cmap=\"gray\")\n",
    "        \n",
    "        plt.subplot(2, 3, j + 2)\n",
    "        plt.imshow(seg[j,0,:, :, 90], cmap=\"gray\")\n",
    "        \n",
    "        plt.subplot(2, 3, j + 3)\n",
    "        plt.imshow(seg[j,1,:, :, 90], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = monai.networks.nets.UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=2,\n",
    "    out_channels=1,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "discriminator = Discriminator().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss functions, optimizer and modeltraining utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions\n",
    "disc_loss = torch.nn.MSELoss(reduction=\"mean\")\n",
    "loss_voxelwise = torch.nn.L1Loss(reduction=\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics and lr scheduler\n",
    "model_metric =  monai.metrics.MAEMetric(reduction='mean')\n",
    "scheduler = ReduceLROnPlateau(optimizer_G, factor=0.9, patience=3, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, 192// 2 ** 4, 240// 2 ** 4,  160// 2 ** 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation loop, training routine adapted from https://github.com/enochkan/vox2vox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_interval = 2\n",
    "best_metric = 100\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = list()\n",
    "metric_values = list()\n",
    "inf = SimpleInferer()\n",
    "writer = SummaryWriter(log_dir='./runs/gan')\n",
    "disc_acc_threshold = 0.8\n",
    "lambda_voxel = 100\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{100}\")\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "        \n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        \n",
    "        # model inputs \n",
    "        input_data, segs, targets = batch_data[\"input\"].to(device), batch_data[\"seg\"].to(device), batch_data[\"img\"].to(device) \n",
    "        \n",
    "        # discriminator ground truths\n",
    "        real_gt = Variable(torch.ones((segs.size(0), *patch))).to(device)\n",
    "        fake_gt = Variable(torch.zeros((segs.size(0), *patch))).to(device)\n",
    "\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        # Real loss\n",
    "        pred_imgs = generator(input_data)\n",
    "        pred_real = discriminator(targets, segs)\n",
    "        loss_real = disc_loss(pred_real, real_gt)\n",
    "        \n",
    "        # Fake loss\n",
    "        pred_fake = discriminator(pred_imgs.detach(), segs)\n",
    "        loss_fake = disc_loss(pred_fake, fake_gt)\n",
    "        # Total loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "        \n",
    "        disc_acc_real = torch.ge(pred_real.squeeze(), 0.5).float()\n",
    "        disc_acc_fake = torch.le(pred_fake.squeeze(), 0.5).float()\n",
    "        disc_acc_total = torch.mean(torch.cat((disc_acc_real, disc_acc_fake), 0))\n",
    "        \n",
    "        if disc_acc_total <= disc_acc_threshold and epoch%2 == 0 : # update discriminator only in alternate epochs\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "                \n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        #  Train Generators\n",
    "        # ------------------         \n",
    "        \n",
    "         # GAN loss\n",
    "        pred_imgs = generator(input_data)\n",
    "        pred_fake = discriminator(pred_imgs, segs)\n",
    "        loss_GAN = disc_loss(pred_fake, real_gt)\n",
    "\n",
    "\n",
    "        # Voxel-wise loss\n",
    "        loss_voxel = loss_voxelwise(pred_imgs, targets)\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + lambda_voxel * loss_voxel\n",
    "        \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()       \n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "\n",
    "        epoch_loss += loss_G.item()\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size\n",
    "        print(f\"{step}/{epoch_len}, train_loss: {loss_G.item():.4f}\")\n",
    "        writer.add_scalar(\"train_loss\", loss_G.item(), epoch_len * epoch + step)\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "    writer.add_scalar(\"Discriminator accuracy training\", disc_acc_total, epoch + 1)\n",
    "    \n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = None\n",
    "            val_epoch_loss = 0\n",
    "            val_step=0\n",
    "            for val_data in val_loader:\n",
    "                val_step += 1\n",
    "                val_input_data, val_segs, val_targets =  val_data[\"input\"].to(device), val_data[\"seg\"].to(device), val_data[\"img\"].to(device)\n",
    "                roi_size = (192,240,160)\n",
    "                val_outputs = inf(inputs=val_input_data, network=generator)\n",
    "                val_loss = loss_voxelwise(val_outputs, val_targets)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]                \n",
    "                # compute metric for current iteration\n",
    "                model_metric(y_pred=val_outputs, y=val_targets)\n",
    "                val_epoch_loss += val_loss.item()\n",
    "            val_epoch_loss /= val_step    \n",
    "            scheduler.step(val_epoch_loss,epoch=epoch+1)\n",
    "            # aggregate the final mean result\n",
    "            metric = model_metric.aggregate().item()\n",
    "            # reset the status for next validation round\n",
    "            model_metric.reset()\n",
    "\n",
    "            metric_values.append(metric)\n",
    "            if metric < best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(generator.state_dict(), \"./models/generator.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                \"current epoch: {} current mean MAE: {:.4f} best mean MAE: {:.4f} at epoch {}\".format(\n",
    "                    epoch + 1, metric, best_metric, best_metric_epoch\n",
    "                )\n",
    "            )\n",
    "            writer.add_scalar(\"val_mean_loss\", val_epoch_loss, epoch + 1)\n",
    "            writer.add_scalar(\"lr\", optimizer_G.param_groups[0]['lr'], epoch + 1)\n",
    "            \n",
    "            \n",
    "            # plot the last model output as GIF image in TensorBoard with the corresponding image and label\n",
    "            plot_2d_or_3d_image(val_targets, epoch + 1, writer, index=0, tag=\"image\")\n",
    "            plot_2d_or_3d_image(val_segs, epoch + 1, writer, index=0, tag=\"label\")\n",
    "            plot_2d_or_3d_image(val_outputs, epoch + 1, writer, index=0, tag=\"output\")\n",
    "\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "writer.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
